# ============================================================================
# MONITORING AND ALERTING CONFIGURATION
# ============================================================================
# Comprehensive monitoring setup for the Survey Insights platform
# ============================================================================

# Prometheus Alert Rules
groups:
  - name: survey_insights_alerts
    rules:
      # Application Health Alerts
      - alert: ApplicationDown
        expr: up{job="survey-insights"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Survey Insights application is down"
          description: "The Survey Insights application has been down for more than 1 minute."

      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.05
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is above 5% for the last 5 minutes."

      # Performance Alerts
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is above 2 seconds."

      - alert: HighMemoryUsage
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes > 0.85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage is above 85%."

      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage"
          description: "CPU usage is above 80%."

      # Database Alerts
      - alert: DatabaseConnectionHigh
        expr: supabase_db_connections_active / supabase_db_connections_max > 0.8
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High database connection usage"
          description: "Database connections are above 80% of maximum."

      - alert: DatabaseSlowQueries
        expr: rate(supabase_db_query_duration_seconds{quantile="0.95"}[5m]) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Slow database queries detected"
          description: "95th percentile query time is above 1 second."

      # AI Service Alerts
      - alert: AIHighCost
        expr: rate(ai_cost_total[1h]) > 10
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High AI service costs"
          description: "AI service costs are above $10/hour."

      - alert: AILowCacheHitRate
        expr: rate(ai_cache_hits_total[5m]) / rate(ai_requests_total[5m]) < 0.5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Low AI cache hit rate"
          description: "AI cache hit rate is below 50%."

      - alert: AIRequestFailure
        expr: rate(ai_requests_failed_total[5m]) / rate(ai_requests_total[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High AI request failure rate"
          description: "AI request failure rate is above 10%."

      # Business Logic Alerts
      - alert: ThemeDiscoveryStalled
        expr: increase(themes_discovered_total[1h]) == 0
        for: 2h
        labels:
          severity: warning
        annotations:
          summary: "Theme discovery appears stalled"
          description: "No themes have been discovered in the last 2 hours."

      - alert: StrategicAlignmentLow
        expr: avg_over_time(strategic_alignment_score[1h]) < 60
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: "Low strategic alignment scores"
          description: "Average strategic alignment score is below 60%."

      - alert: HighUnreviewedThemes
        expr: themes_unreviewed_total > 50
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "High number of unreviewed themes"
          description: "More than 50 themes are waiting for PM review."

# Grafana Dashboard Configuration
dashboard:
  title: "Survey Insights Platform Monitoring"
  tags: ["survey-insights", "monitoring"]
  panels:
    - title: "Application Health"
      type: "stat"
      targets:
        - expr: up{job="survey-insights"}
      thresholds:
        - value: 0
          color: "red"
        - value: 1
          color: "green"

    - title: "Request Rate"
      type: "graph"
      targets:
        - expr: rate(http_requests_total[5m])
      yAxes:
        - label: "Requests/second"

    - title: "Response Time"
      type: "graph"
      targets:
        - expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))
      yAxes:
        - label: "Seconds"

    - title: "Error Rate"
      type: "graph"
      targets:
        - expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m])
      yAxes:
        - label: "Error Rate"
        - max: 1

    - title: "Database Connections"
      type: "graph"
      targets:
        - expr: supabase_db_connections_active
      yAxes:
        - label: "Connections"

    - title: "AI Service Costs"
      type: "graph"
      targets:
        - expr: rate(ai_cost_total[1h])
      yAxes:
        - label: "Cost/hour"

    - title: "Theme Discovery Rate"
      type: "graph"
      targets:
        - expr: rate(themes_discovered_total[1h])
      yAxes:
        - label: "Themes/hour"

    - title: "Strategic Alignment Distribution"
      type: "histogram"
      targets:
        - expr: strategic_alignment_score
      bucketSize: 10

# Alertmanager Configuration
alertmanager:
  global:
    smtp_smarthost: 'localhost:587'
    smtp_from: 'alerts@survey-insights.com'

  route:
    group_by: ['alertname']
    group_wait: 10s
    group_interval: 10s
    repeat_interval: 1h
    receiver: 'web.hook'
    routes:
      - match:
          severity: critical
        receiver: 'critical-alerts'
      - match:
          severity: warning
        receiver: 'warning-alerts'

  receivers:
    - name: 'web.hook'
      webhook_configs:
        - url: 'http://localhost:5001/'

    - name: 'critical-alerts'
      email_configs:
        - to: 'oncall@survey-insights.com'
          subject: 'CRITICAL: {{ .GroupLabels.alertname }}'
          body: |
            {{ range .Alerts }}
            Alert: {{ .Annotations.summary }}
            Description: {{ .Annotations.description }}
            {{ end }}

    - name: 'warning-alerts'
      email_configs:
        - to: 'devops@survey-insights.com'
          subject: 'WARNING: {{ .GroupLabels.alertname }}'
          body: |
            {{ range .Alerts }}
            Alert: {{ .Annotations.summary }}
            Description: {{ .Annotations.description }}
            {{ end }}

# Uptime Monitoring
uptime:
  endpoints:
    - name: "Main Application"
      url: "https://survey-insights.vercel.app"
      interval: 60s
      timeout: 30s
      expected_status: 200

    - name: "API Health Check"
      url: "https://survey-insights.vercel.app/api/health"
      interval: 60s
      timeout: 10s
      expected_status: 200

    - name: "Strategy API"
      url: "https://survey-insights.vercel.app/api/admin/strategy/current"
      interval: 300s
      timeout: 10s
      expected_status: 401  # Expected to return 401 without auth

    - name: "Themes API"
      url: "https://survey-insights.vercel.app/api/admin/themes"
      interval: 300s
      timeout: 10s
      expected_status: 400  # Expected to return 400 without company_id

# Log Monitoring
logs:
  sources:
    - name: "Application Logs"
      path: "/var/log/survey-insights/app.log"
      patterns:
        - pattern: "ERROR"
          severity: "error"
        - pattern: "WARN"
          severity: "warning"

    - name: "Database Logs"
      path: "/var/log/survey-insights/db.log"
      patterns:
        - pattern: "deadlock"
          severity: "critical"
        - pattern: "timeout"
          severity: "warning"

# Performance Baselines
baselines:
  response_time:
    p50: 500ms
    p95: 2s
    p99: 5s

  error_rate:
    warning: 1%
    critical: 5%

  availability:
    warning: 99.5%
    critical: 99%

  ai_cache_hit_rate:
    warning: 60%
    critical: 40%

# Notification Channels
notifications:
  slack:
    webhook_url: "${SLACK_WEBHOOK_URL}"
    channel: "#alerts"
    username: "Survey Insights Bot"

  email:
    smtp_server: "smtp.gmail.com"
    smtp_port: 587
    username: "${EMAIL_USERNAME}"
    password: "${EMAIL_PASSWORD}"

  pagerduty:
    integration_key: "${PAGERDUTY_INTEGRATION_KEY}"

# Maintenance Windows
maintenance:
  - name: "Weekly Database Maintenance"
    schedule: "0 2 * * 0"  # Every Sunday at 2 AM
    duration: "2h"
    suppress_alerts: true

  - name: "Monthly AI Model Updates"
    schedule: "0 3 1 * *"  # First day of every month at 3 AM
    duration: "4h"
    suppress_alerts: true
